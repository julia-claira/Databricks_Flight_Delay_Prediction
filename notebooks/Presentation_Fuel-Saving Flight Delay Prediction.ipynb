{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67172df1-b5c5-475c-acf9-2c9fe0c330b1",
     "showTitle": true,
     "title": "Phase 3"
    }
   },
   "source": [
    "# Fuel-Saving Flight Delay Prediction: A Data-Driven Approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ccee906-7cba-451e-8588-a7b7a69c40a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Project Team and Responsibilities\n",
    "\n",
    "Phase 1 Leader: Angelina Wedemeyer <br>\n",
    "Phase 2 Leader: Sudha Ravi Kumar Javvadi<br>\n",
    "Phase 3 Leaders: Claira Kauffmann, Priya Reddy\n",
    "\n",
    "Current Credit Assignment Plan:\n",
    "| Name              | Email                  | Photo | Responsibilities      |\n",
    "| ----------------- | ---------------------- | ----- | ----------------------- |\n",
    "| Angelina Wedemeyer  | angelina@ischool.berkeley.edu | ![Angelina's Photo](https://drive.google.com/uc?id=1mQJQeE0mFlXy4IeNDIUZ2npowT17nSYO) | <table> <thead> <tr> <td>Phase</td> <td>Task</td> <td>Percent Time</td> </tr> </thead> <tbody> <tr> <td>1</td> <td>EDA</td> <td>90</td> </tr> <tr> <td>1</td> <td>Exploring methods to create join</td> <td>10</td> </tr> <tr> <td>2</td> <td>Custom join, troubleshooting, scaling, and testing</td> <td>70</td> </tr> <tr> <td>2</td> <td>Logistic Regression</td> <td>15</td> </tr> <tr> <td>2</td> <td>Presentation</td> <td>5</td> </tr> <tr> <td>2</td> <td>Features for next phase</td> <td>5</td> </tr> <tr> <td>2</td> <td>Phase 2 Notebook</td> <td>5</td> </tr> <tr> <td>3</td> <td>Feature Engineering</td> <td>15</td> </tr> <tr> <td>3</td> <td>Logistic Regression</td> <td>10</td> </tr> <tr> <td>3</td> <td>Gradient Boost</td> <td>15</td> </tr> <tr> <td>3</td> <td>Hyperparameter Tuning</td> <td>25</td> </tr> <tr> <td>3</td> <td>Debugging</td> <td>25</td> </tr> <tr> <td>3</td> <td>Phase 3 Notebook</td> <td>10</td> </tr> </tbody> </table> |\n",
    "| Claira Kauffmann  | julia.claira@ischool.berkeley.edu      |<img src=\"https://drive.google.com/uc?id=16bG71XAQMNdlQtEZKo6D3z54SXbzUzpp\" width=\"100\"> | <table> <thead> <tr> <td>Phase</td> <td>Task</td> <td>Percent Time</td> </tr> </thead> <tbody> <tr> <td>1</td> <td>EDA</td> <td>100</td> </tr> <tr> <td>2</td> <td>EDA</td> <td>10</td> </tr> <tr> <td>2</td> <td>Statistical interactions between terms</td> <td>20</td> </tr> <tr> <td>2</td> <td>Convert categorical Features to One-hot encoding</td> <td>20</td> </tr> <tr> <td>2</td> <td>Mean calculations of categorical Features</td> <td>20</td> </tr> <tr> <td>2</td> <td>Exponential Weighting Average in Cross Validation</td> <td>10</td> </tr> <tr> <td>2</td> <td>Random Forests Implementation</td> <td>10</td> </tr> <tr> <td>2</td> <td>Presentation</td> <td>5</td> </tr> <tr> <td>2</td> <td>Phase 2 Notebook</td><td>5</td> </tr>  <tr> <td>3</td> <td>Feature Engineering</td> <td>10</td> </tr> <tr> <td>3</td> <td>Random Forest</ttd> <td>10</td> </tr> <tr> <td>3</td> <td>EDA</td> <td>15</td> </tr> <tr> <td>3</td> <td>Hyperparameter Tuning</td> <td>25</td> </tr> <tr> <td>3</td> <td>Debugging</td> <td>25</td> </tr> <tr> <td>3</td> <td>Phase 3 Notebook</td> <td>10</td> </tr> </tbody> </table>|\n",
    "| Priya Reddy | priyareddy@berkeley.edu   | ![Priya's Photo](https://drive.google.com/uc?id=1pc91Pp31afrHt82-dxUyNCBzxF59TS55) | <table> <thead> <tr> <td>Phase</td> <td>Task</td> <td>Percent Time</td> </tr> </thead> <tbody> <tr> <td>1</td> <td>EDA</td> <td>100</td> </tr> <tr> <td>2</td> <td>Setup baseline pipeline</td> <td>15</td> </tr> <tr> <td>2</td> <td>Train-test split</td> <td>5</td> </tr> <tr> <td>2</td> <td>Grid-Search implementation</td> <td>25</td> </tr> <tr> <td>2</td> <td>Linear SVM</td> <td>15</td> </tr> <tr> <td>2</td> <td>SVC with grid-search</td> <td>30</td> </tr> <tr> <td>2</td> <td>Presentation</td> <td>5</td> </tr> <tr> <td>2</td> <td>Phase 2 Notebook</td> <td>5</td> </tr> <tr> <td>3</td> <td>Setup Final Pipeline</td> <td>25</td> </tr> <tr> <td>3</td> <td>Linear Regression SVM</td> <td>20</td> </tr> <tr> <td>3</td> <td>SVC Hyperparameter Tuning</td> <td>20</td> </tr> <tr> <td>3</td> <td>GAP Analysis</td> <td>5</td> </tr> <tr> <td>3</td> <td>Presentation</td> <td>15</td> </tr> <tr> <td>3</td> <td>Phase 3 Notebook</td> <td>15</td> </tr> </tbody> </table> |\n",
    "| Sudha Ravi Kumar Javvadi   | javvadis@ischool.berkeley.edu    | <img src=\"https://drive.google.com/uc?id=1RpZNiHYzz6S31PlTsk7n_J8iRmHQTawI\" width=\"100\"> | <table> <thead> <tr> <td>Phase</td> <td>Task</td> <td>Percent Time</td> </tr> </thead> <tbody> <tr> <td>1</td> <td>EDA</td> <td>90</td> </tr> <tr> <td>1</td> <td>Extracting meaning information from date-columns</td> <td>10</td> </tr> <tr> <td>2</td> <td>More EDA on validating important features</td> <td>5</td> </tr> <tr> <td>2</td> <td>Pluggable Cross-Validation function</td> <td>25</td> </tr> <tr> <td>2</td> <td>Decision Trees</td> <td>5</td> </tr> <tr> <td>2</td> <td>MLP</td> <td>25</td> </tr> <tr> <td>2</td> <td>Presentation</td> <td>5</td> </tr> <tr> <td>2</td> <td>Pluggable grid-search function</td> <td>25</td> </tr> <tr> <td>2</td> <td>Validating the cross-validation and grid-search functions working</td> <td>5</td> </tr> <tr> <td>2</td> <td>Phase 2 Notebook</td> <td>5</td> </tr> <tr> <td>3</td> <td>MLP Model</td> <td>25</td> </tr> <tr> <td>3</td> <td>Optimizing Pipeline</td> <td>15</td> </tr> <tr> <td>3</td> <td>Hyperparameter Tuning</td> <td>25</td> </tr> <tr> <td>3</td> <td>Debugging</td> <td>25</td> </tr> <tr> <td>3</td> <td>Phase 3 Notebook</td> <td>10</td> </tr> </tbody> </table> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8300e57-e349-435b-bc02-6a97a0f2382d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Phase  Summary: \n",
    "\n",
    "Our project aims to predict the flight delays to aid airlines to reduce their carbon footprints and so improve the air quality. We later evaluate our models using F2-score. Using the F2-score will allow us to put a focus on preventing the false negative of failing to predict a delay, and also factor in the cost of the false positives of predicting a delay when there is none. This achieves our goal of focusing on improving air quality and also respects the business requirements that our airline partners must run as a business.  In the early phases of the project our best baseline was from a Linear SVM model that had an F2-score of 0.57 . In this final phase, we conducted more EDA over four years of data, experimented with different models identifying their optimal hyper parameters, and tested out final models. The best preforming model in our testing of this phase was the Gradient Boost model with a validation F2-score of 0.694 . Evaluated on the blind dataset the final Gradient Boost model produced a F2-score of 0.636\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6062502-2a9e-4b72-8c98-9ec3b638c85a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Project Abstract\n",
    "\n",
    "**Motivation:**\n",
    "To improve local air quality at airports and help airlines reach toward green aviation, our team is creating a tool to predict flight delays. We believe the market is ripe for our tool as we are in an age of carbon emmision reduction. Though much of pollution in aviation occurs in the air, studies have found that CO2 emission from delays of aircraft at the taxiing phase have a significant influence in local air quality (source). We expect airlines to partner with us to both reduce their emisions and also to reduce their costs of delays. In the recent report on Air Traffic the FAA has found that in FY2022 “the number of Core 30 airport departure delays of at least 15 minutes rose significantly, by 44.8 percent”(source). In addition, they found that from 2012 to 2019 the cost of these delays to the airlines has been steadily increasing, going from $5.7 Billion to $8.3 Billion. As a result it is important that airlines carefully track and plan for delays so that they can manage their operational costs efficiently while reducing their environmental impact. Towards this objective we will leverage machine learning techniques and historical flight and weather data from the Department of Transportation and the National Oceanic and Atmospheric Administration repository respectively to create a reliable prediction system. These models will be evaluated by their F-2 score as the believe both precision and recall to be important, but value recall more as we dont want to fail to predict a delayed flight. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1GRNDKgsAy_KLJzt5mN6CilCTr5Na_9-U\" width=500><br>\n",
    "(The data for the line graph depicting the global CO2 emissions from commercial aviation between 2013 and 2019 is sourced from the paper [\"Climate Impacts of Aviation\"](https://www.faa.gov/air_traffic/by_the_numbers/media/Air_Traffic_by_the_Numbers_2023.pdf))\n",
    "\n",
    "**Process:**\n",
    "- Phase 1: Define the objective and constraints of the project. Explore a three month subset of the dataset. Identify potentially valuable data and candidate models.\n",
    "- Phase 2: Complete an in-depth exploratory data analysis (EDA) on one year of training data. Test baseline models on simplified pipeline. Create a custom join of the data.\n",
    "- Phase 3: Enhance data with engineered features. Complete exploratory data analysis on four years of training data. Experiment with top candidate models. Select final predictive model and evaluate the held out blind test set of data.\n",
    "\n",
    "**Deliverable:**\n",
    "We will produce a flight delay prediction tool that operated on our custom pipeline, such that when intaking unseen flight and weather data will be able to predicting flight delays. The successful implementation of our predictive model will have several benefits. By predicting flight delays, airlines can optimize flight routes, schedules, and speeds. This will result in decreased fuel consumption, reduced operational costs, and a smaller carbon footprint. This not only improves profitability but also aligns with sustainability goals by reducing greenhouse gas emissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3180332-d37a-4fe1-940e-e21d3d735eaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Project Timeline\n",
    "Below is the timeline for our project  \n",
    "\n",
    "![Phase 3 Gantt Chart](https://drive.google.com/uc?id=1H6bVFjwLrXb-yRSxtHEHvH3lOfJojspp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cdd4aac-0395-45dd-b6c0-d68fd0745440",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Custom Join\n",
    "\n",
    "While completing EDA on the provided OTPW dataset we identified some errors that led us to complete our own join(code [here](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1537154891801768/command/1537154891801864) ) of the flight and weather data. Though the data on the custom join may be more correct on the parameters we were analyzing, there is a significant drop in the number of records in the custom join. Therefore in in Phase 2 we conducted all modeling on the provided joined dataset. In Phase 3 of this project we will run a test on both datasets to see whether the custom join or provided joined data aids in a better model prediction.\n",
    "\n",
    "An example of the error we identified in the provided joined dataset, is that there were cases where the flight data was not connected to the correct weather data. In this example below, the flights departing at local time 00:50, and 00:25 from airports PDX and SEA are connected to the local timestamps 05:50 and 02:25. This is incorrectly mapped in the provided data, but with our join we were able to connect it to the correct timestamp in the weather data. As we rely on the weather data to predict the flight delays, our aim is to have weather data as precise as possible.\n",
    "\n",
    "| Flight Origin Airport   |   Local Time Estimated Departure HHmm | Pre-joined Local Departure   | Pre-joined UTC Departure   | Custom Join Local Departure   | Custom Join UTC Departure   |\n",
    "|:------------------------|--------------------------------------:|:-----------------------------|:---------------------------|:------------------------------|:----------------------------|\n",
    "| LAX                     |                                   115 | 2015-01-01T01:15:00 ✅       | 2015-01-01T09:15:00 ✅     | 2015-01-01T01:15:00Z ✅       | 2015-01-01T09:15:00Z ✅     |\n",
    "| PDX                     |                                    50 | 2015-01-01T05:50:00 ❌       | 2015-01-01T13:50:00 ❌     | 2015-01-01T00:50:00Z ✅       | 2015-01-01T08:50:00Z ✅     |\n",
    "| SEA                     |                                    25 | 2015-01-01T02:25:00 ❌       | 2015-01-01T10:25:00 ❌     | 2015-01-01T00:25:00Z ✅       | 2015-01-01T08:25:00Z ✅     |\n",
    "\n",
    "The data was joined in four stages, and was checkpointed after each stage. The first stage was to join the weather stations provided with nearest airports and their distances, and airports from the U.S. Department of Transportation, these were joined using the zipcodes of the stations and the airports. The second stage was to join in the utc offset for each airport. This proved difficult as provided data such as an airports state is not a sufficiently precise geographic region to assign timezones, as some states such as Nebraska are split nearly evenly into two time zones. To accomplish timezone assignment we used the data from [National Weather Service](https://www.weather.gov/gis/ZoneCounty), which provided us the latitude and longitude of each FIPS centroid and whether or not it used daylight savings time. Time Zones were assigned by joining the airport's data to its nearest FIPS region using the longitude and latitude of each dataset.  The third stage was to join the station and airport data to the flight data which was also made available from U.S. Department of Transportation, this join used the airport code of the origin flight. The fourth stage was to join the enriched flight data to the weather data using the station id, the weather data is from the National Oceanic and Atmospheric Administration repository. This was completed with a restriction that the weather data had to be recorded at a timestamp two hours before the scheduled departure time. This fourth join was the most expensive, to make it the most efficient as possible, we prefiltered the weather data each time to only have the stations contained in the airport and station data, we also selected only the columns that were required for each step, and broadcasted the flight data. The chart below shows the time and resources used for the stage four joins of the flight and weather data on different time periods.\n",
    "\n",
    "| Time Frame                 | Time to Join   |   Clusters  | Count Rows in Joined Data    |\n",
    "|:---------------------------|:---------------|------------:|:-----------------------------|\n",
    "| 3 Months, Jan - March 2015 | 1.02 minutes   |          64 | 913,915                      |\n",
    "| 1 Year, 2015               | 1.02 minutes   |          64 | 3,808,118                    |\n",
    "| 5 Years, 2015 - 2019       | 14.73 minutes  |          67 | 20,817,851                   |\n",
    "\n",
    "The diagram below is a simplified graph of how the datasets were joined. \n",
    "\n",
    "![Custom Join Diagram](https://drive.google.com/uc?id=1ZiiBEGixl_o0g_mX0h90jZGBCrZUt4hJ)\n",
    "\n",
    "### Looking Forward (2020-present)\n",
    "This custom join was limited to the years 2015 - 2019. As flight patterns have changed drastically since then, we are interested in future research that includes data from the years 2020, 2021, and 2022. These years were impacted by the pandemic, and will require another iteration of EDA and feature engineering to capture the nuances. At this stage we have collected the weather data for these years (stored [here](https://drive.google.com/drive/folders/1W0vx7WaU_B-losFS_vblr6nmO2O10iae?usp=sharing)) but have not yet begun the process of conducting the custom join. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a88e83-96af-4eb3-9cfe-835b1f3bfaac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Tasks\n",
    "\n",
    "In this final phase of our project, we are focusing on completing our exploratory data analysis (EDA), optimizing our machine learning models through grid search, conducting training and blind validation, selecting the best model, and performing a final test on our reserved test set. Additionally, we will conduct a comprehensive gap analysis to identify any remaining challenges or opportunities for improvement.\n",
    "\n",
    "The tasks for Phase 3 are outlined as follows: \n",
    "\n",
    "1. Final EDA: Complete an exhaustive exploratory data analysis to uncover any remaining trends and patterns in the full dataset.\n",
    "2. Model Optimization: Use grid search to fine-tune hyperparameters across all models.\n",
    "3. Training and Blind Validation: Train models with the best hyperparameters and perform blind validation to assess performance.\n",
    "4. Model Selection: Choose the best performing model based on training and validation results.\n",
    "5. Final Testing: Test the selected model on the reserved test set to evaluate its real-world applicability.\n",
    "6. Gap Analysis: Conduct a final analysis to identify any discrepancies and areas for improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dd49ea2-294c-42ee-8cb8-ce1f9563b1da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## EDA\n",
    "\n",
    "[Link to EDA Code](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1184043630589527)\n",
    "\n",
    "In this final phase of our Exploratory Data Analysis (EDA), we focused on the On-Time Performance of Flights and Weather dataset (OTPW). Focusing on this subset allowed for an efficient analysis of various data aspects (though it potentially limited our view of longer-term trends over several years).\n",
    "\n",
    "\n",
    "\n",
    "**Output of Interest:**\n",
    "Our primary focus was on departure delays, defined as planes taking off 15 minutes late or more. This aligns with our environmental concerns regarding planes waiting on the tarmac. The key feature here is **DEP_DEL15**, a binary column where 1 indicates a delay of 15 minutes or more, and 0 signifies on-time or minor delays.\n",
    "<p>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1eNF--WB9NtSO_tjCXcFZlw9_c7OedOPe\" width=400>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1mOR-8DN9leCAadVymMB3v184bAxsaESD\" width=700>\n",
    "\n",
    "Late aircrafts emerged as a primary cause. Carrier delays and NAS issues, which encompass traffic and airport operations, also contribute significantly. While weather-related delays are less frequent, however their unpredictable nature likely interplays with these other factors.\n",
    "\n",
    "**Addressing Missing Data:**\n",
    "For the initial phase of our EDA was identified columns for elimination to make our dataset more manageable. Starting with columns with substantial data gaps,  we removed columns that had more than 80% missing data. This included daily and monthly aggreagates. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=12rtSiQDZF7fpoAD7dx2plyQJYwZFwyEB\" width=750>\n",
    "\n",
    "While daily and monthly data may be informative, we chose to focus on hourly data, believing it would allow us to construct more flexible time windows. For the hourly weather data, which exhibited fewer missing values, we employed a feed-forward approach for gap filling, leveraging previous satellite readings.\n",
    "\n",
    "\n",
    "**Redundant Data:**\n",
    "We also addressed redundancy in the dataset. By grouping columns with over 99% collinearity, we streamlined our analysis, reducing 40 columns down to 7. Eliminating this excess allowed us to focus our EDA.<p>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1j6dkoq_1xK30y1f-R9cOgcm98MEjLaaR\" width=600>\n",
    "\n",
    "\n",
    "**Distribution and Balance**:\n",
    "The dataset consists of categorical flight variables, and continuous weather metrics. The select histograms below show our continous, hourly weather metrics appear to be close to normally distrubuted. The number of departures drop off at the end of the day, while it remains consistently high for arrivals until midnight.\n",
    "\n",
    "We noticed an imbalance in our output of interest, DEP_DEL15. <br>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1AXW3nF9CQMV2bFzo0CCKSic45RXUVEiN\" width=700>\n",
    "\n",
    "This imbalance introduced complications to our pipeline because it can skew a model's ability to predict the less frequent events. To counteract this, we downsampled to balance the dataset. This reduced our sample size, but we believe the remaining data is sufficient for training purposes. For future stages, we're considering data augmentation techniques to further enhance our model's performance.\n",
    "\n",
    "| Dataset Overview:                   |Before Balancing| After Balancing  |\n",
    "| ----------------------------------- | -------------- |----------------- |\n",
    "| Number of Positive Class (delay)    | 933,343         | 933,343          |\n",
    "| Number of Negative Class (no delay) | 3,716,140      | 933,266          |\n",
    "\n",
    "It should be noted that the classes are now mostly balanced, but they are not equal. According to Apache Spark's API documentation, \"[Sampling] is not guaranteed to provide exactly the fraction specified of the total count of the given DataFrame.\"\n",
    "\n",
    "**Insights from Flight Data:**\n",
    "A more detailed analysis of flight data revealed that delays often occurred in the later hours, suggesting a cumulative effect throughout the day. We would like to explore this phenomenon further.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1HO7aOz1GnQbdKIf9DccwD9tJ3aj-glGI\" width=750>\n",
    "\n",
    "Interestingly, smaller airports exhibited higher percentages of delays, possibly due to limited resources and smaller aircraft types being more susceptible to technical issues.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1SLPaDrbL3-ysdX5c5YmUYssobF-k5NR_\" width=750>\n",
    "<br>\n",
    "\n",
    "However, upon a deeper investigation, the trend isn't straightforward. We observed that the smallest airports have the most delays, followed by a decrease in mid-sized airports, and then an increase again in larger ones. While our current models use features like 'ORIGIN' to capture some of these size-related differences, future models might benefit from being tailored to different airport scales. Such an approach could lead to more precise predictions of flight delays.\n",
    "<br>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1m9J8bggqDlNEXmS320Ku9d-6bhQussiM\" width=750>\n",
    "\n",
    "A few airports had either 100 percent delays or 0. These are anomolies, and we see that they only have one or two flights over a period of five years. Upon investigation, these appear to be government or military airports for various reasons had singular private flights routed through them. We may remove them from the dataset, but we also do not believe that they are likely to affect our analysis.\n",
    "<br>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1DiJdlQlXr-Jw4bXUuGr8T_Ozt3tDidig\" width=440>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Feature Interactions**:\n",
    "Next, we used statistical principles to multiply features, aiming to uncover complex interactions between features. For example, precipitation likely affects flights differently depending on the time of year. \n",
    "\n",
    "While some of our data didn't meet the normality assumption of a Pearson Correlation, it still provided a valuable overview, though it may be skewed. Through this method we found a likely correlation between carrier and departure time with respect to departure delays. (Note: we used the length of the delay for this correlation instead of our binary output value to allow for more nuance in our measurements.)<p>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1EiKAUangi8Rm0LrFys36Qgl5TQMCo0Uq\">\n",
    "\n",
    "For our feature interactions, we used mean encoding for the categorical data, allowing us to multiply them with the continous, weather metrics. Importantly, we calculated the means in the coding using a rolling window to avoid data leakage.\n",
    "\n",
    "**Feature Selection**:\n",
    "We used Lasso regression and Random Forest for further refinement by examinging feature importance in those models. Here are engineered feature interactions are highlighted in orange and pink, highlighting their value. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1LEJ-RpGJLMUgodutoJ6piphSlDOK5abV\" width=960><br>\n",
    "\n",
    " Notably, an interaction between carrier and departure time topped the list, though again, we notice variations in feature importance depending on airport size. <p>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=19GtoR-JqSrnnsg7qRIXFgphJgexcXsIB\" width=960><br>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1dpYTn05EEIUinFbxQfbnNJoNomgxNHpX\" width=960><br>\n",
    "\n",
    "We plan to further investigate these interactions in subsequent analyses, however, for establishing our baseline, we chose to rely solely on existing features, with the exception of applying one-hot encoding to categorical data.\n",
    "\n",
    "[Link to EDA Code](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1184043630589527)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2314e70-01c9-4527-a852-48bb2e31f097",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    " For all the models processed in this phase we used the below final set of features. \n",
    "\n",
    "|    | Feature Name              | Data Type     |\n",
    "|---:|:--------------------------|:--------------|\n",
    "|  0 | CARRIERVec                | OneHotEncoded |\n",
    "|  1 | ORIGINVec                 | OneHotEncoded |\n",
    "|  2 | ORIGIN_STATE_NMVec        | OneHotEncoded |\n",
    "|  3 | ORIGIN_WACVec             | OneHotEncoded |\n",
    "|  4 | DESTVec                   | OneHotEncoded |\n",
    "|  5 | DEST_STATE_NMVec          | OneHotEncoded |\n",
    "|  6 | STATIONVec                | OneHotEncoded |\n",
    "|  7 | ELEVATIONVec              | OneHotEncoded |\n",
    "|  8 | REPORT_TYPEVec            | OneHotEncoded |\n",
    "|  9 | SOURCEVec                 | OneHotEncoded |\n",
    "| 10 | QUARTER                   | Int           |\n",
    "| 11 | MONTH                     | Int           |\n",
    "| 12 | DAY_OF_MONTH              | Int           |\n",
    "| 13 | DAY_OF_WEEK               | Int           |\n",
    "| 14 | YEAR                      | Int           |\n",
    "| 15 | FL_DATE                   | Int           |\n",
    "| 16 | CRS_ARR_TIME              | Int           |\n",
    "| 17 | CRS_DEP_TIME              | Int           |\n",
    "| 18 | DATE                      | Int           |\n",
    "| 19 | LATITUDE                  | Float         |\n",
    "| 20 | LONGITUDE                 | Float         |\n",
    "| 21 | HourlyAltimeterSetting    | Float         |\n",
    "| 22 | HourlyDewPointTemperature | Int           |\n",
    "| 23 | HourlyDryBulbTemperature  | Int           |\n",
    "| 24 | HourlyPrecipitation       | Float         |\n",
    "| 25 | HourlyPressureChange      | Float         |\n",
    "| 26 | HourlyPressureTendency    | Int           |\n",
    "| 27 | HourlyRelativeHumidity    | Int           |\n",
    "| 28 | HourlySeaLevelPressure    | Float         |\n",
    "| 29 | HourlyStationPressure     | Float         |\n",
    "| 30 | HourlyVisibility          | Float         |\n",
    "| 31 | HourlyWetBulbTemperature  | Int           |\n",
    "| 32 | HourlyWindDirection       | Int           |\n",
    "| 33 | HourlyWindGustSpeed       | Int           |\n",
    "| 34 | HourlyWindSpeed           | Int           |\n",
    "| 35 | DEP_DEL15                 | Int           |\n",
    "\n",
    "\n",
    "#### New Features \n",
    "\n",
    "In addition to the aforementioned interaction features mentioned in the EDA, we have also engineered (code [here](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2798664673125986/command/1537154891795908)) a set of features that will be added to the data in Phase 3. The features were motivated by the goal to uncover data within the dataset, for example by counting the number of flights delayed at an airport which is not readily availble in the data in its present state. Additionally we added in features using outside data sources that would inform our models of holidays, the historically peak days of delay. \n",
    "\n",
    "\n",
    "1.  **RecentHourlyPrecipitationDelta**: The change in recorded precipitation over the last 12 hrs, not including weather from two hours prior. \n",
    "\n",
    "2.  **RecentHourlyRelativeHumidityDelta**: The change in recorded humidity over the last 12 hrs, not including weather from two hours prior. \n",
    "\n",
    "3.  **HolidayTravel**: The days surrounding and including five holidays, Independence Day, Memorial Day, Labor Day, Thanksgiving, and Christmas Day.\n",
    "\n",
    "4. **TAIL_NUM_PREV_DELAY**: The number of times the same plane was delay on previous legs of its journey over the last 12 hrs, not including data from two hours prior. \n",
    "\n",
    "5. **ORIGIN_DELAY_SUM**: The number of delays at the origin airport of the flight, the count is summed over the last 12 hrs, not including data from two hours prior.\n",
    "\n",
    "\n",
    "#### Feature Interactions: \n",
    "\n",
    "In predictive modeling, the relationship between input variables (features) and the target variable (e.g., flight delay) can be complex and non-linear. Simply considering each feature independently may not capture the full picture. This is where feature interactions come into play. We explored various interactions that we plan to implement in our more advanced models.\n",
    "\n",
    "By multiplying certain features together, we aim to capture complex interactions, leading to more accurate predictions in our flight delay models.\n",
    "\n",
    "\n",
    "1. **DayOfWeek_CRS_DEP_TIME**: `DAY_OF_WEEK_mean * CRS_DEP_TIME`\n",
    "   - *Interaction between the day of the week and scheduled departure time, potentially capturing weekly patterns in departure times.*\n",
    "\n",
    "2. **Carrier_CRS_DEP_TIME**: `OP_CARRIER_mean * CRS_DEP_TIME`\n",
    "   - *Captures the relationship between specific carriers and their scheduled departure times, which might indicate carrier-specific scheduling patterns.*\n",
    "\n",
    "3. **Month_CRS_DEP_TIME**: `MONTH_mean * CRS_DEP_TIME`\n",
    "   - *Reflects how departure times vary across different months, potentially capturing seasonal variations in flight schedules.*\n",
    "\n",
    "4. **Carrier_DayOfWeek**: `OP_CARRIER_mean * DAY_OF_WEEK_mean`\n",
    "   - *Examines the interaction between carriers and days of the week, possibly indicating carrier-specific weekly operational patterns.*\n",
    "\n",
    "5. **Origin_CRS_DEP_TIME**: `ORIGIN_mean * CRS_DEP_TIME`\n",
    "   - *Explores the relationship between the origin airport and the scheduled departure time, which could highlight airport-specific traffic patterns.*\n",
    "\n",
    "6. **Precipitation_at_DepTime_and_Origin**: `HourlyPrecipitation * CRS_DEP_TIME * ORIGIN_mean`\n",
    "   - *Investigates how precipitation at the time of departure and at the origin airport might influence delays.*\n",
    "\n",
    "7. **Visibility_OriginAirport**: `HourlyVisibility * ORIGIN_mean`\n",
    "   - *Looks at how visibility conditions at the origin airport might impact flight operations.*\n",
    "\n",
    "8. **WindSpeed_Origin**: `HourlyWindSpeed * ORIGIN_mean`\n",
    "   - *Assesses the impact of wind speed at the origin airport on flight schedules.*\n",
    "\n",
    "9. **Temperature_Month**: `HourlyDryBulbTemperature * MONTH_mean`\n",
    "   - *Explores the interaction between ambient temperature and the time of year, potentially capturing seasonal weather patterns.*\n",
    "\n",
    "10. **Precipitation_Carrier**: `HourlyPrecipitation * OP_CARRIER_mean`\n",
    "    - *Examines how different carriers might be affected by precipitation, possibly indicating carrier-specific resilience to weather conditions.*\n",
    "\n",
    "11. **WindSpeed_Visibility**: `HourlyWindSpeed * HourlyVisibility`\n",
    "    - *Investigates the combined effect of wind speed and visibility on flight conditions.*\n",
    "\n",
    "12. **DewPoint_Humidity**: `HourlyDewPointTemperature * HourlyRelativeHumidity`\n",
    "    - *Looks at the relationship between dew point temperature and humidity, which can be critical for understanding weather conditions.*\n",
    "\n",
    "13. **Temp_WindSpeed**: `HourlyDryBulbTemperature * HourlyWindSpeed`\n",
    "    - *Explores how the combination of temperature and wind speed might affect flight conditions.*\n",
    "\n",
    "14. **PressureTendency_Altimeter**: `HourlyPressureTendency * HourlyAltimeterSetting`\n",
    "    - *Assesses the interaction between atmospheric pressure changes and altimeter settings, important for flight navigation.*\n",
    "\n",
    "15. **Visibility_SkyConditions**: `HourlyVisibility * HourlySkyConditions`\n",
    "    - *Examines how visibility and sky conditions together might impact flight schedules.*\n",
    "\n",
    "16. **WindDir_GustSpeed**: `HourlyWindDirection * HourlyWindGustSpeed`\n",
    "    - *Looks at the relationship between wind direction and gust speed, which can be crucial for understanding wind-related delays.*\n",
    "\n",
    "17. **SeaLevel_StationPressure**: `HourlySeaLevelPressure * HourlyStationPressure`\n",
    "    - *Investigates the interaction between sea-level pressure and station pressure, important for weather predictions.*\n",
    "\n",
    "18. **Precipitation_WetBulbTemp**: `HourlyPrecipitation * HourlyWetBulbTemperature`\n",
    "    - *Explores how precipitation and wet bulb temperature together might influence flight conditions.*\n",
    "\n",
    "19. **PressureChange_DewPoint**: `HourlyPressureChange * HourlyDewPointTemperature`\n",
    "    - *Assesses the combined effect of atmospheric pressure changes and dew point temperature on weather conditions.*\n",
    "\n",
    "20. **Humidity_Temperature**: `HourlyRelativeHumidity * HourlyDryBulbTemperature`\n",
    "    - *Investigates the interaction between humidity and temperature, which can be significant for understanding atmospheric conditions.*\n",
    "\n",
    "Certain interactions, like carrier and delay-time, showed promise when we looked for feature importance using Lasso Regression. We plan to explore more complex interactions for the next phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcf75bc9-533c-45f7-9ee1-96c655feb698",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Leakage Management\n",
    "\n",
    "Our plan to minimize leakage for this project was mainly targeted to two risk areas. First the risk of selecting or imputing data that was not available until after our cutoff of two hours before the flight's scheduled departures. The second major risk was the potential of seeing test data during the training process. Listed below are the precautions we took at each stage.\n",
    "\n",
    "|    | Stage                     | Leakage Mitigation Strategy                                                                                          |\n",
    "|---:|:--------------------------|:---------------------------------------------------------------------------------------------------------------------|\n",
    "|  0 |   Raw Data                |  Include only pre flight data                                                                                        |\n",
    "|  1 |   Weather Data Join       |  Join data that is at a minimum 2 hours before flight scheduled departure                                            |\n",
    "|  2 |   Feature Engineering     |  Engineered time based features are set to a time lag of 2-12 hours before scheduled takeoff                         |\n",
    "|  3 |   Pipeline Data Prep      |  When scaling features, we fit and transform the train set, but only transform the validation set                    |\n",
    "|  4 |   Model Training          |  The categorical variables are mean encoded during each fold of the cross validation.                                |\n",
    "|  5 |   Test                    |  To ensure valid testing, the data was divided into 3 sections, train, validation, and a blind test set              |\n",
    "|  6 |   Evaluate                |  Once an optimal model was chosen after all experiments, a single test was evaluated on the blind test set           |\n",
    "\n",
    "\n",
    "Acknowledged leaking:\n",
    "- During step 4, Model Training when then entire fold is mean encoded the mean is computed from the whole training set. To mitigate this in the future we can implement a rolling mean.\n",
    "- For step 6, Evaluate we ended up testing two models on the final blind test set. This was due to time constraints, and an unexpectedly preformant MLP model. For these reasons we ran both the Gradient Boost and MLP model on the final blind test set, which we acknowledge is a case of leaking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc8cb92e-d99b-429f-b4be-b57997e77d62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Model Tests\n",
    "\n",
    "#### Experimental Modeling Pipeline\n",
    "\n",
    "Once our EDA on the 5 year dataset was completed we were able to narrow our dataset to just features we found to be most relevant. We also cleaned the dataset a little in order to move forward with modeling the delays. This cleaning process includeded:\n",
    "- Dropping the columns that had too many nulls in then (as shown in the EDA above)\n",
    "- Using a feedforward method to impute nulls in columns that we did keep\n",
    "- Dropping columns that would cause data leakage \n",
    "- Dropping columns that we found to be irrelevant (based on our EDA above)\n",
    "  - Further information on these columns can be found in the feature engineering section of this report\n",
    "\n",
    "With this cleaned and filtered dataset we then began the process of selecting our final ML model. To do this we ran several experiments with 7 models – logistic regression, linear SVM, decision tree, gradient boosted decision tree (GBDT), random forest, and multi-layer perceptron to test not only the different models but also different features and different combinations of hyperparameters. To conduct these experiments systematically we used the following pipeline.\n",
    "\n",
    "![Phase 3 Experimental Modeling Pipeline](https://drive.google.com/uc?id=1U1gHUE26mOqpcCKLF0xjRXx8rcjTtC0e)\n",
    "\n",
    "*The red circles on the above diagram indicate stages in the pipeline where we checkpointed our data to save time and computational resources. We ended up checkpointing the `blind_test_1Y`, `training_3Y` and `validation_8M` data. We also had a checkpointing step in our cross-validation cycle for once the feature engineering was completed.*\n",
    "\n",
    "1. First we split our `custom_5Y_join` dataset into a `training_4Y` and a `blind_test_1Y` set. The `training_4Y` data consisted of all data from 2015-2018 and the `blind_test_1Y` set consisted of all of the 2019 data. \n",
    "\n",
    "2. We split the `train` data once more into a `training_3Y` and `validation_8M` set. The `validation_8M` set consisted of the last 8 months of the 2018 data. Our plan was to train our models on the `training_3Y` data, and then validate on the `validation_8M` set. We would choose among the models based on the metrics associated with this training and validation steps and only once we had decided on a final model would we run our model on the `blind_test_1Y` set. In this way we could avoid leakage and keep the `blind_test_1Y` set as untouched as possible. \n",
    "\n",
    "3. Once the data is split we began our cross-validation with grid search step. In particular we chose to do rolling cross-validation for the time series data to avoid leakage. For each fold of the cross-validation step the model was trained on the training 80% and validated on the remaining 20% of the fold. We used this cross-validation step to check for consistency in performance across time periods as well as to just see model performance (in terms of weighted average F2 scores). The results from this validation process also informed how we tuned and added to the features in our dataset. In addition to this CV step we also conducted our grid-search step at the same time. We repeated this CV process for each combination of hyperparameters that we wanted to test and we selected the combination of hyperparameters that had the best weighted average F2 score across folds. \n",
    "\n",
    "4. We also had an intermediate downsampling and feature engineering step within this cross-validation cycle. We downsample the data  because in our initial EDA we found there was a class imbalance with a 5:1 ratio of non-delayed to delayed flights. To address this imbalance, we chose to downsample the majority class of data.  In addition we also did our feature engineering steps (creating interaction features, mean-encoding, casting to float etc) during the cross-validation cycle. Doing these steps within the cross-validation step also helped with minimizing data leakage as we cross-validated the data.\n",
    "\n",
    "5. Finally once we adjusted our features to get a cross-validation performance we were happy for each model we then trained on the entirety of the 3 year training dataset (`training_3Y`) and validated on our reserved 2018 data (`validation_8M`). Using this weighted validation F2 we were able to select our final model – Gradient Boosted Decision Trees. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Final Pipeline\n",
    "![Phase 3 Final Modeling Pipeline](https://drive.google.com/uc?id=15TTh25Dy_KbTs6OQ9lxaYUbaNj13ibTn)\n",
    "\n",
    "\n",
    "Once the model is decided on our final modeling pipeline is very similar to the experimental modeling pipeline with the removal of the cross-validation grid search step. The tuned model is trained on the entirety of the feature engineered and downsampled `training_4Y` data (2015-2018) and then evaluated on the `blind_test_1Y` (2019) set. From there the model is ready to be used. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Future Modifications\n",
    "\n",
    "While the current pipeline we have works well and results in a well performing model, there are still a few modifications that we might experiment with going forwards. \n",
    "\n",
    "For one, when training our models we uniformly applied a standard scaler to all of our features to normalize them. This is something that we may adjust in future iterations of our modeling pipeline, as we can see in the EDA that the feature distributions are not all normal (many have right tailed distributions). It might improve our model's performance to instead log-transform some of the features to deal with this tailed distribution and then apply a scaler.\n",
    "\n",
    "Another change we may make to future experimental modeling pipelines is in our feature engineering step. In this phase we trained each model on the same set of features. This included all of our derived and added feature. But it's possible that the inclusion of some features in certain models (ex. non-linear interaction features in the MLP model) is resulting in subpar performance. In future experiments we plan to adjust this feature engineering step to be more model specific. \n",
    "\n",
    "In addition, in this phase of the project we did not implement any early-stopping criteria for our models, and regardless we did not see much overfitting among the models. But in future experiments, especially ones where we may be working with more complex models (such as Neural Networks), additional engineered features, and additional flight and weather data (2020-2022) where the training time and resource cost is much higher, we may choose to implement some form of early stopping whereby we stop training and save the model if the improvement in our target metric over different epochs is not above some threshold. This would help us conserve resources (compute resources and time) as we explore more models and conduct further experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc46cb2e-3733-4376-aefd-d8b0b3b4971b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Experimental Results\n",
    "\n",
    "The major metric we used to evaluate our models is F2-score. We used Grid-Search to fine-tune our hyper-parameters. As mentioned in the previous sections, just tuning a couple of parameters increased the training/evaluation time exponentially, so we carefully selected just a few parameters for each model to tune through independent research. \n",
    "\n",
    "In the following sections we provide a quick walk-through of our baseline result, followed by our grid-search experiments for each model, and model comparisons to select our final model. Based on the experiments and further validating the results, we found that Gradient Boosted Decision Tree classifier performed best which was selected as our final model.\n",
    "\n",
    "Code for these graphs can be found [here](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1012234209199560/command/1012234209199781)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c0e674-66ab-4da0-ad58-dee9542742f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Logistic Regression Classifier\n",
    "\n",
    "![Logistic-Regression Grid-Search](https://drive.google.com/uc?id=1_paObVCdZyE-q9f0cdKDaPRH1vf61_2N)\n",
    "\n",
    "It took 1.2 hours to complete Grid-Search on the following hyper-parameters:\n",
    "\n",
    "* `fitIntercept: [False, True]`\n",
    "* `regParam: [0.01, 0.1]`\n",
    "\n",
    "From the above heatmaps, we can notice that most of the hyper-parameters are providing similar F2-scores. But, we notice that the best F2-score is when there is a reqularization parameter of value `0.01` and there is an intercept fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b56948e9-20fa-47f7-9ac0-b2cfabb9bbcb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Decision Trees Classifier\n",
    "\n",
    "![Decision-Trees Classifier Grid-Search](https://drive.google.com/uc?id=1wuU8dgSu4mntp_5wNwB_TgziVQ5Bcjlj)\n",
    "\n",
    "It took 3.96 hours to perform grid-search on the following hyper-parameters:\n",
    "\n",
    "* `maxBins: [32, 64, 128]`\n",
    "* `maxDepth: [5, 10, 15]`\n",
    "* `impurity: ['entropy', 'gini']`\n",
    "\n",
    "From the above heatmaps, we can notice that as we increase the depth, F2-score on training data increases but there is no good pattern on the validation data. Similarly, we do not notice any patterns by increasing the number of bins for classification. But, we noticed the best F2-score for maxDepth of 15, maxBins of 64, and gini as impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f22f60d-65bc-42ed-ab1e-b39ee4fe7d22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Linear SVM Classifier\n",
    "\n",
    "![Linear SVM Grid-Search](https://drive.google.com/uc?id=1Ik9M19T1jiCEMon1lIsJDzD1e1RwIrEb)\n",
    "\n",
    "It took 4.3 hours to perform grid-search on the following hyper-parameters:\n",
    "\n",
    "* `regParam: [0.01, 0.1, 1.0]`\n",
    "* `maxIter: [100,200]`\n",
    "* `tol: [1e-4, 1e-6]`\n",
    "\n",
    "From the above heatmaps, we noticed that increasing the regularization coefficient has a better generalizability for a very low tolerance value and large number of iterations. But, we could not find more patterns in it. Also, the best F2-score on validation data was observed with tolerance value of `1e-6`, regularization coefficient of `1.0` and `200` maximum iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4ddbdb9-5f2f-48dd-9a6c-affffa1e6c7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Random Forests Classifier\n",
    "\n",
    "![random-forest-grid-search](https://drive.google.com/uc?id=1pzCdBqvND_K8ExJo1o4pyK4EQZVRsSiH)\n",
    "\n",
    "It took 6.5 hours to perform grid-search on the following hyper-parameters:\n",
    "\n",
    "* `maxDepth: [3, 7, 10]`\n",
    "* `maxBins: [32, 64, 128]`\n",
    "* `numTrees: [5, 10, 15]`\n",
    "\n",
    "We can notice from the heatmaps that for `32` number of maximum bins, F2-score on training increases as the maximum depth is increased irrespective of the number of trees. But, this do not generalize well on the validation data. In fact, the F2-score keeps decreasing on the validation data as the maximum depth is increased. This shows that random forests is able to overfit on the training data and is unable to provide that good generalizability. Still the model performed best when the maximum depth is `3` for `32` number of maximum bins and for `5` number of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cb6da20-4bfa-4490-8d76-bf2727089e39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Multi-Layer Perceptron Classifier\n",
    "\n",
    "![mlp-grid-search](https://drive.google.com/uc?id=1E1Kqzc8hACQz_-y7VI7L1O0FDc_rWWqT)\n",
    "\n",
    "It took 8.44 hours to perform grid-search on the following hyper-parameters:\n",
    "\n",
    "* `solver: [\"l-bfgs\", \"gd\"]`\n",
    "* `maxIter: [25, 50, 75, 100]`\n",
    "\n",
    "Note that each combination of the above parameters is futher evaluated on the below set of network architectures:\n",
    "\n",
    "* `[59, 32, 8, 2]`\n",
    "* `[59, 32, 16, 8, 4, 2]`\n",
    "* `[59, 32, 32, 8, 8, 2]`\n",
    "\n",
    "The number of inputs are `59` which made the first layer to contain 59 nodes. Also, we doing a binary classification and so the last layer has 2 nodes. From the above heatmaps, we can notice that increasing the complexity of the model by adding more layers and nodes, helped with the evaluation on training data. But, this did not generalize well on the validation data. We can notice that for the most complex architecture, F2-score on the validation data is much better than on the training data. We are skeptical about the results. But, the best performing hyper-parameters included using gradient descent as the optimization algorithm and running for 50 number of iterations and using `[59, 32, 32, 8, 8, 2]` as the network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "587014b9-43a5-47a4-94cc-bc375d326621",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Gradient Boosted Decision Trees Classifier\n",
    "\n",
    "![GBDT Hyperparameter Heatmap](https://drive.google.com/uc?id=1RA1jxxTdmShOv5fAvGWdin6quou3fD7r)\n",
    "\n",
    "Here is an overview of our grid-search results for our final model, the gradient boosted decision tree. \n",
    "\n",
    "It took 6.8 hours to perform grid-search on the following hyper-parameters:\n",
    "\n",
    "* `stepSize: [0.01, 0.1]`\n",
    "* `maxIter: [5, 10, 50, 100]`\n",
    "* `maxDepth: [2, 4, 6]`\n",
    "\n",
    "For this model we tested combinations of different step sizes, max iterations, and max depths. We can see the changes in the model’s training F2 score with different combinations of these hyperparameters in this heatmap. For example we can see that across both step sizes, as maxIter increases, in general the F2 score also increases but increasing the depth did not help after `4` as the max depth. Overall we found that setting the step size to `0.1`, the maxIter parameter to `100`, and the maxDepth parameter to `4` resulted in the highest validation F2 score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "554003fb-fe31-4352-a286-b7aa7172debb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Discussion\n",
    "\n",
    "While accuracy of positive predictions and completeness of positive predictions are both important to our project, we give a higher preference to the completeness of positive predictions. And so <b>F2 measure is our success metric</b>. \n",
    "\n",
    "In below sections we present our evaluation metrics using cross-validation on training data (2015-2017) and our validation data (2018). Note that for the below results, we ran each model with the best hyperparameters that we selected from our grid-search experiments above. For this cross-validation, we followed the same approach we used to evaluate our baseline models in phase 2.\n",
    "\n",
    "* We divided the training into 4 folds. \n",
    "* Now, for each fold, let's say that training data is `i`, then validation data `i+1`.\n",
    "* We applied a exponentially weighted average to compute the final F2-score as explained below:\n",
    "  * We multiplied each fold with a factor such a that metric further in the past have an exponential decay of `0.1`. The rationale behind this being we give more preference to the results in the near past than the ones in far past.\n",
    "  * Finally, the F2-metric score is the average of the above values.\n",
    "\n",
    "\n",
    "\n",
    "| Model | Selected | Train F2-score | Validation F2-score | Time Taken (without CV) | Notebook Link |\n",
    "|---------------------------------|------------------------------------------------------------------------------------------|----------------|---------------------|------------|------------------------------------------------------------------------------------------------------------|\n",
    "| Logistic Regression             | Maximum Iterations: 10                                                                   | 0.62           | 0.63                | 7.11 min   | [Link](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319532104/) |\n",
    "| Decision Trees                  | Maximum Depth: 10<br> Maximum Bins: 128<br> Impurity: Gini                               | 0.63           | 0.65                | 7.56 min   | [Link](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319532342/) |\n",
    "| Linear SVM                      | Maximum Iterations: 200<br>Convergence Tolerance: 1e-6<br>Regularization Parameter: 1.0  | 0.61           | 0.66                | 8.61 min   | [Link](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319532457/) |\n",
    "| Random Forest                   | Maximum Depth: 15<br>Maximum Bins: 25<br>Impurity: Entropy<br>Number of Trees: 10        | 0.67           | 0.65                | 10.77 min  | [Link](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319532712/) |\n",
    "| Multi-layer Perceptron          | Layers: [59, 32, 16, 8, 4, 2]<br>Maximum Iterations: 50<br>Solver: Gradient Descent (GD) | 0.83           | 0.95                | 8.65 min   | [Link](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319532383/) |\n",
    "| Gradient Boosted Decision Trees | Maximum Iterations: 100<br>Maximum Depth: 4<br>Step Size: 0.1                            | 0.65           | 0.68                | 9.71 min   | [Link](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319532066/) |\n",
    "\n",
    "As can be seen from the above table, each model performed really well after including a larger training dataset, features extraction, and fine-tuning the hyperparameters. Infact, all of them performed much better than the baseline models. The validation f2-score is better than the train f2-score except for random forests in which case we think it overfitted more on the training data. Also, this validation f2-score keeps improving as we move from simpler models to more complex models.. \n",
    "\n",
    "\n",
    "We also compared these results against our baseline results (seen below) from Phase 2 to make sure that we were seeing an improvement with the changes we were making. \n",
    "\n",
    "| Baseline Model         | Train F2-score | Validation F2-score | Time Taken (with CV) |\n",
    "| ---------------------- | -------------- | ------------------- | ---------- |\n",
    "| Logistic Regression    | 0.58           | 0.54                | 1.5 hrs    |\n",
    "| Decision Trees         | 0.57           | 0.54                | 17.92 mins |\n",
    "| Random Forest          | 0.54           | 0.49                | 15.71 mins |\n",
    "| Linear SVM             | 0.55           | 0.57                | 2.96 hrs   |\n",
    "| Multi-layer Perceptron | 0.56           | 0.55                | 17.10 mins |\n",
    "\n",
    "We can see that across all models there was an improvement in F2 score both on the training and validation data, and so we knew we could move forward with our updated models and dataset. \n",
    "\n",
    "We noticed a huge jump in F2 score as with the MLP model and as a result it appears to be the best performing model on our validation data. To validate that these results are valid, we obtained the confusion matrix for MLP results on the validation data and noticed that MLP is predicting everything as delayed as shown in the below confusion matrix. This reduced the precision but resulted a recall of `1.0`. Since this is no better than a human evaluator labelling every flight as delayed, we decided to not consider MLP as the best model for our business case, even though it had the best validation F2-score.\n",
    "\n",
    "| Actual\\Predicted | Predicted Positive | Predicted Negatives |\n",
    "|------------------|--------------------|---------------------|\n",
    "| Actual Positives | 2654841            | 0                   |\n",
    "| Actual Negatives | 609543             | 0                   |\n",
    "\n",
    "Because of this, we selected our next best model, the <b>Gradient-Boosted Decision Trees classifier, as our final model</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfb1c77d-0d5d-4d33-9d11-0fb05bb65312",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Gap Analysis\n",
    "\n",
    "The final results for this model's performance can be seen below:\n",
    "\n",
    "| Data                | Size       | Precision     | Recall     | F2      |\n",
    "| ------------------- | ---------- |-------------- |----------- |-------- |\n",
    "| Train (2015-2018)   | 5,722,547  |    0.65       |  0.65      | 0.65    |\n",
    "| Test (2019)         | 4,902,154  |    0.89       |  0.59      | 0.64    |\n",
    "\n",
    "We can see that the model performed fairly well on the test data with an F2 score 0.64, and that the model did not overfit on the training data. We see slightly lower F2-score on test than expected from our experiments but it is slightly less than training F2-score which is expected.\n",
    "\n",
    "| Actual           | Predicted Positive | Predicted Negatives |\n",
    "|------------------|--------------------|---------------------|\n",
    "| Actual Positives | 2375392            | 1623543             |\n",
    "| Actual Negatives | 298188             | 605031              |\n",
    "\n",
    "Just to ensure that this model did not become as metric driven as MLP, we checked the confusion matrix as well and noticed the above distribution. Turns out that the model ended predicted more observations as positives but slightly lesser than what MLP did. But, the number of false negatives predicted is lesser than the number of false positives which is what we wanted our model to do. So, overall, the model performed decent enough and we peformed further analysis on the predictions.\n",
    "\n",
    "Once we verified these results we decided to take a closer look at the predictions to try and see where our model was struggling to predict delays correctly.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1-o2otdLSc3HVoQVGUuWbyetteElV7HcG\" width=\"45%\" />\n",
    "<img src=\"https://drive.google.com/uc?id=1qpWCT4urz-2sjPG_1_-FJG1RTSv2O1yX\" width=\"45%\">\n",
    "\n",
    "In the plot above to the left we can see the ratio of incorrect predictions (#incorrect/total predictions) per month in the test data. It looks like the  incorrect predictions peak towards the starting, middle, and very end of the year. This overlaps mostly with the summer and winter breaks when people travel very often. We also plotted the percent of correctly predicted delays (#predicted positive/ #actual delays) per month on the right. We can see in the plot to the right that the model does better at correctly predicting the positive class during the summer and winter breaks which is in-line with the understanding of the left graph. Both the above graphs might actually show a high-level understanding on the delays which matches more on the travel patterns of the general public. So, having a controlled blocking on holidays when downsampling for training might help the model perform better.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1UrUVCT6Ovqhh0FFOh0m5dDx8TW2HKh45\"  width=\"45%\"/>\n",
    "<img src=\"https://drive.google.com/uc?id=1hfMjQA3kzj2V7rQg9V8U450J4O7TssCc\" width=\"45%\" />\n",
    "\n",
    "The above graph on the left shows the percentage of correctly predicted delays across the months of 2019 and is facetted by airport size.  As can be observed from the graph, the model is able to predict the positive class very well when it comes to huge airports but not that much as the airport size decreases. It's difficult to numerically define this relationship as the size of the airport is a qualitative measure that we came up with, relating to the number of flights passing through, rather than a numerically defined metric (this is something we may look into updating in furture models). It's possible that that this difference can be attributed to a class imbalance in the data (seen in the plot to the right). The model would have seen a large number of observations for huge airports, as there are a large number of flights scheduled at these airports, and much fewer observations for smaller airports. This imbalance may have actually been exacerbated by the indiscriminate downsampling that we did to deal with the output variable class imbalance.  Using this analysis we can improve the performance of future models by blocking the downsampling on the size of the airports, which would give a more balanced data for models to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaa409e0-fe86-4479-9829-cc83d2f65abd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Challenges and Future Work\n",
    "\n",
    "Challenges faced had a range of causes from implementation decisions, to challenges inherent in large time series data. Two examples are how we chose to approach our modeling pipeline for all models, and the value of data over time having a larger impact as the data size increases. \n",
    "\n",
    "We choose to create a single pipeline for each team member to clone and test a subset of models. This was efficient as it kept the team aligned as per how to execute each stage of the pipeline. The challenges we faced were due to the higher communication cost to ensure each bug fix was propagated through to each team member's cloned branch. Though this level of collaboration came at a cost we would approach future stages in a similar fashion as it ensures knowledge sharing, though we may keep a better record of a change log.\n",
    "\n",
    "This final phase saw a dramatic increase in size of data. We did encounter some scaling concerns such as the increased time it took to complete model training. This was solved by implementing a checkpoint method to make a more expedient dag at the time of processing. Another item we began to see with the larger dataset was what appeared to be the way the value of data deprecated over time. Though we choose to model equally weighing the entire training dataset for this phase, we may try a different approach in the future.\n",
    "\n",
    "Areas of interest for future work include:\n",
    "\n",
    "* Recent events have more predictive capability than events farther in the past as features evolve over time in time-series data. So, creating an ensemble model that uses exponential decay to weight more recent events over ones further in the past might increase the generalizability of the model.\n",
    "* Perform downsampling of training data on size of the airport and also on months containing more number of holidays. We expect that the this is improve the overall model performance. But, seeing that the large difference in mis-classifications for huge versus other sized airports, we also suggest creating separate models for different airport sizes.\n",
    "* We can engineer more features, including special weather events, or vacation times to help even linear models  generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c26afc5-8c32-4fa0-aef8-3479e105a2ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this phase of the project we were able to conduct our final EDA on the entirety of the training dataset, construct our modeling pipeline, and run tests with several different models to tune the features we used as well as the hyperparameters for each model. We then were able to select the final model that we went forward with for this project -- Gradient Boosted Decision Tree Classifier model. \n",
    "\n",
    "On this note, we would like to conclude that using this model to detect flight delays could have many benefits:\n",
    "\n",
    "* If used in 2017 it would have cut fuel emissions from 5520 tones to 3202 tones.\n",
    "* Using this model allows airlines to optimize their fuel consumption and save money.\n",
    "* Providing these delay predictions to customers can help them better manage their time.\n",
    "\n",
    "We see huge potential for our work and are pleased to share this predictive model with airlines for use in predicting flight delays!\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase 3 _ Team 2-3 _ Fuel-Saving Flight Delay Prediction",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
